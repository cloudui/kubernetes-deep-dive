# Networking

House rules:

- All nodes have to be able to talk to each other
- All Pods can talk to one another without NAT
- Every Pod gets its own IP address

![Screen Shot 2022-06-21 at 10.17.59 AM.png](Networking%20b4a230b3847842b8b3a70a4204bbd491/Screen_Shot_2022-06-21_at_10.17.59_AM.png)

- Each pod sees itself as its own internal IP address, and every other pod sees it as that too
    - There is no internal/public IP difference
    - All pods can see each other
- Each Node is allocated a subset IP range

# Networking Services

How do other pods that rely on others know which ones are healthy or unhealthy?

![Screen Shot 2022-06-21 at 10.21.43 AM.png](Networking%20b4a230b3847842b8b3a70a4204bbd491/Screen_Shot_2022-06-21_at_10.21.43_AM.png)

- Network abstraction that ensures you are talking to a healthy instance

## Services

Every service gets a name and an IP

- Stable IP and name: they do not change, ever
- Every cluster can have a DNS service
    - Every pod knows how to use it
- Service as a load balancer / proxy
- Can use a label selector to choose which pods are served

![Screen Shot 2022-06-21 at 10.24.08 AM.png](Networking%20b4a230b3847842b8b3a70a4204bbd491/Screen_Shot_2022-06-21_at_10.24.08_AM.png)

- New objects that fit the label are automatically added to the Search service / endpoint object
- Endpoint target will always have the same name as the Search service it’s associated with

## Service Types

All do essentially the same thing: building networks, different ones have different use cases

### LoadBalancer

- Integrates with public cloud platform
- Must have public API, good for Azure, GCP, and AWS
- Azure and AWS create a Nodeport to service their own load balancers!

### NodePort

![Screen Shot 2022-06-21 at 10.58.45 AM.png](Networking%20b4a230b3847842b8b3a70a4204bbd491/Screen_Shot_2022-06-21_at_10.58.45_AM.png)

- Also gets cluster IP
- Gets cluster-wide **port**
- Accessible from outside of cluster
    - Access via node IP + NodePort value at the end

### ClusterIP (default)

- The default option
- Gets own IP
- Only accessible from within cluster

# The Service Network

Service gets unique IP, but it’s not the pod or the node network, which each have their separate IP ranges

![Screen Shot 2022-06-22 at 11.15.02 PM.png](Networking%20b4a230b3847842b8b3a70a4204bbd491/Screen_Shot_2022-06-22_at_11.15.02_PM.png)

How traffic get to it:

- Every node has a kube-proxy running on it (daemon-set), one per node
- It writes IPVS/IPTables rules
    - Essentially converts the original destination (svc address) to a new address (pod address)
    - Kind of like NAT
        
        ![Screen Shot 2022-06-22 at 11.17.02 PM.png](Networking%20b4a230b3847842b8b3a70a4204bbd491/Screen_Shot_2022-06-22_at_11.17.02_PM.png)
        

## How traffic is sent (Pod to Service)

![Screen Shot 2022-06-22 at 11.21.46 PM.png](Networking%20b4a230b3847842b8b3a70a4204bbd491/Screen_Shot_2022-06-22_at_11.21.46_PM.png)

Pod sends to DNS, which gets an IP that is not on the pod network (SVC)

- Sends it to the network interface, which doesn’t know where it is either
- Sends it to the default gateway, a Linux bridge called CBR0 (similar to Docker0), but for K8s → Custom Bridge Zero
- Sends the packets upstream again, to the node’s eth0
    - Get processed by the kernel of the host
    - The kernel applies the rule written in the IP table, which gives it the destination IP

## Defaults

Kube-proxy IPTABLES Mode is the default (since K8s 1.2)

It doesn’t scale well

- Not designed for load balancing

Kube-proxy IPVS Mode instead, default now

- Stable since K8s 1.11
- Uses Linux kernel IP Virtual Server
- Native Layer-4 load balances
- Supports more algorithms (load)

# Demo:

[https://github.com/ACloudGuru-Resources/Course_Kubernetes_Deep_Dive_NP/tree/master/lesson-networking](https://github.com/ACloudGuru-Resources/Course_Kubernetes_Deep_Dive_NP/tree/master/lesson-networking)

```bash
# Get nodes
kubectl get nodes

# Create deployment, apply it to cluster
kubectl apply -f ./ping-deploy.yml

# Get the deployments
kubectl get deploy

# Get the pods, wide view
kubectl get pods -o wide

# See the CIDR blocks of pods
kubectl get nodes -o jsonpath='{.items[*].spec.podCIDR}'

# Enter a pod
kubectl exec -it $(CONTAINER_NAME) bash

# Get the service IP of NodePort...
curl hello-svc:8080 
curl $(INDIVIDUAL_IP):30001 # NodePort port appended

# Watch the svc update process
kubectl get svc --watch
```