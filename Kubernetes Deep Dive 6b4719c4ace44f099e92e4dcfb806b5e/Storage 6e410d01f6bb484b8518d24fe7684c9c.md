# Storage

Volume purpose: decoupling storage from pods

- Otherwise storage lives & dies with pods

Pods lay claims to volumes and mount them. Can be used by multiple pods at the same time

- Native File & Block storage

### Requirements

- Speed
- Replication
- Resiliency

Kubernetes only runs as a service. You need your own storage backend, whether it be S3 or EBS or whatever

Container storage interface (CSI) lets your system interface with the storage backend

### Kubernetes Volume Subsystem

- Persistent Volume (PV)
- Persistent Volume Claim (PVC)
- Storage Class (SC)

CSI is out of tree

- All the code is decoupled from K8s and is on its own release cycle with other developers
- Developers can make CSI plugins

![Untitled](Storage%206e410d01f6bb484b8518d24fe7684c9c/Untitled.png)

PVC - resource in API, object in cluster

YAML files must match on pvc and CV

- If using GCP, the disk (or whatever) must exist when deploying the cluster

```bash
kubectl get pv
```

### [Access Modes](../Kubernetes%20Deep%20Dive%206b4719c4ace44f099e92e4dcfb806b5e.md)

- RWO: ReadWriteOnce
    - One node per volume
- RWM: ReadWriteMany
    - Only File Systems generally support (NFS, essentially)
    - Many nodes write to same storage endpoint
- ROM: ReadOnlyMany
    - Read Replica…kind of

### Storage Clases

- Standard
- SSD

### Reclaim Policy

- Retain
    - Sticks around in a separate lifecycle
    - Must be manually deleted
- Delete
    - Delete when released, also deleted on storage backend

## Example YAML config:

PV Claim

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
	name: pvc1
spec: 
	accessModes:
	- ReadWriteOnce
	storageClassName: ssd
	resources:
		requests:
			storage: 20Gi
```

PV

```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
	name: pv1
spec:
	accessModes:
	- ReadWriteOnce
	storageClassName: ssd
	capacity:
		storage: 20Gi
	persistentVolumeReclaimPolicy: Retain
	gcePersistentDisk:
		pdName: uber-disk
```

Pod Spec:

```yaml
apiVersion: v1
kind: Pod
metadata: 
	name: volpod
spec: 
	volumes:
	- name: data
		psersistentVolumeClaim:
			claimName: pvc1
	containers:
	- image: ubuntu:latest
		name: ubuntu-ctr
		command:
		- /bin/bash
		- "-c"
		- "sleep 60m"
		imagePullPolicy: IfNotPresent
		volumeMouns:
			- mountPath: /data
				name: data
```

PV & PVC specs must match

- It can still work technically as long as PVC storage is less than PV storage

# Storage Classes & Dynamic Provisioning

The previous example seems simple, but it is very manual. It isn’t scalable.

## AWS Example

```yaml
kind: StorageClass
apiVersion: storage.k8s.io/v1 # decoupled from v1 monolith
metadata:
	name: fast
provisioner: kubernetes.io/aws-ebs # EBS volume
parameters:
	type: io1
	zones: eu-west-1a
	iopsPerGB: "10"
---
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
	name: slow
	annotations:
		storageclass.kubernetes.io/is-default-class: "true" # setting default
provisioner: kubernetes.io/aws-ebs
parametes:
	type: gp2
	zones: eu-west-1a
reclaimPolicy: Retain
mountOptions:
	- debug
```

- Creates the resource instead of you having to define it and then manually write it into the YAML

## Some commands

```bash
kubectl get pv
kubectl get pvc

kubectl describe sc $(NAME)

# When you want to provision a storage now...
kubectl apply -f pvc.yml
# It will make a resource based on SCs
# Creates your PV and PVC
```

Uses default storage classes if not specified in YAML